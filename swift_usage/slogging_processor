#!/usr/bin/env python

from ConfigParser import ConfigParser

import os.path
import sys
import datetime
import csv
import zlib
import hmac
import hashlib
from swift_usage.utils import db_connect # requires pymongo
from swift_usage.utils import swift_client

conf_file = ""
conf = ConfigParser({ # defaults
            "slogging-container":"log_processing_data",
            "collect-till-delta":"3", # you will never have logs up to date.  search for logs up to this many hours ago.
            "misses-till-delta":"10", # retry missing hours till this many hours ago, then abandon. (MUST be larger than collect-till-delta)
            "enable-logging":"False",
            "key-salt":"89cd705d9ed8161855d573c09794e191",
            "log-file":"/var/log/swift/swift-usage.log" # make sure you have write permission for this file (or run slogging_processor as root)
        })

if os.path.exists("/etc/swift/swift-usage.conf"):
    conf_file = "/etc/swift/swift-usage.conf"
    
conf.read(conf_file)

try:
    swift_auth_url = conf.get('slogging-processor', 'swift-auth-url')
except:
    sys.exit("slogging_processor: config required, [slogging-processor] -> swift-auth-url")
try:
    swift_user = conf.get('slogging-processor', 'swift-user')
except:
    sys.exit("slogging_processor: config required, [slogging-processor] -> swift-user")
try:
    swift_key = conf.get('slogging-processor', 'swift-key')
except:
    sys.exit("slogging_processor: config required, [slogging-processor] -> swift-key")
    
# load optional config if it exists in conf file.
slogging_container = conf.get('slogging-processor', 'slogging-container')
dt_now_delta = int(conf.get('slogging-processor', 'collect-till-delta'))
dt_miss_delta = int(conf.get('slogging-processor', 'misses-till-delta'))
logging = conf.getboolean('DEFAULT', 'enable-logging')
key_salt = conf.getboolean('DEFAULT', 'key-salt')
log_file = conf.get('DEFAULT', 'log-file')

if logging:
    log = open(log_file, 'a')


db = db_connect.db # connect to the usage db
swift = swift_client.Connection(swift_auth_url, swift_user, swift_key) # create a swift client for accessing data in swift.

# get the list of objects in the slogging container.
try:
    container_headers, container_objects = swift.get_container(slogging_container) # grab the usage objects from slogging
except:
    sys.exit("slogging_processor: check container '%s' to ensure you have logs." % slogging_container)

log_names = sorted([obj["name"] for obj in container_objects if ".csv.gz" in obj["name"]]) # create a sorted list of the slogging logs


# function to process a dt and see if there is an associated log, if so, process it...
def process_dt(dt, from_miss=False):
    file_substr = dt.strftime("%Y/%m/%d/%H/") # date substring for the current dt to match the incoming .csv.gz file
    file_arry = [name for name in log_names if file_substr in name] # there should only be one match (we only look at the first)
    if len(file_arry) > 0: # we have a matching log for this hour.
        obj_headers, obj_data = swift.get_object(slogging_container, file_arry[0]) # get the log data
        d = zlib.decompressobj(16 + zlib.MAX_WBITS) # setup the decompresser | http://stackoverflow.com/questions/2423866/python-decompressing-gzip-chunk-by-chunk/2424549#2424549
        csv_data = csv.reader(d.decompress(obj_data).split('\n'), delimiter=',') # get the csv data from the log
        header_row = True # first row is the header, then this will be set to False
        headers = []
        for data in csv_data: # go through line at a time.
            if header_row: # populate the array which will be they keys in the usage object
                headers = data
                header_row = False
            else: #data row, so process the data.
                account = data[1]
                mm_key = "%s%s" % (data[0][0:4], data[0][5:7]) # format: YYYY/MM/DD HH:MM:SS => YYYYMM
                dt_key = "%s%s%s%s" % (data[0][0:4], data[0][5:7], data[0][8:10], data[0][11:13]) # format: YYYY/MM/DD HH:MM:SS => YYYYMMDDHH
                existing_monthly_usage = db.usage.find_one({"account":account, "month":mm_key})
                if existing_monthly_usage: # there is an existing account with usage, add this usage to it.
                    existing_hourly_usage = [usage for usage in existing_monthly_usage['usage'] if usage['hour'] == dt_key]
                    if existing_hourly_usage: # update this hours data using this new data.
                        if logging:
                            log.write("UPDATING hour '%s' for '%s'...\n" % (dt_key, account))
                        existing_obj = existing_hourly_usage[0]['data']
                        usage_obj = {}
                        for i, header in enumerate(headers[2:]):
                            usage_obj[header] = data[i+2]
                        updated_obj = {}
                        for k, v in usage_obj.items():
                            if k == 'bytes_used':
                                updated_obj[k] = (int(existing_obj[k]) + int(usage_obj[k])) / 2
                            else:
                                updated_obj[k] = int(existing_obj[k]) + int(usage_obj[k])
                        if logging:
                            log.write(str({"hour":dt_key, "data":updated_obj})+"\n")
                        db.usage.update({"account":account, "month":mm_key, "usage.hour":dt_key}, {"$set":{"usage.$.data":updated_obj}})
                    else: # add this data as this hours data.
                        if logging:
                            log.write("INSERTING hour '%s' into '%s'...\n" % (dt_key, account))
                        usage_obj = {}
                        for i, header in enumerate(headers[2:]):
                            usage_obj[header] = data[i+2]
                        if logging:
                            log.write(str({"hour":dt_key, "data":usage_obj})+"\n")
                        db.usage.update({"account":account, "month":mm_key}, {"$push":{"usage":{"hour":dt_key, "data":usage_obj}}})
                else: # create the user and seed it with the current data.
                    if logging:
                        log.write("CREATING usage account '%s' starting from '%s'...\n" % (account, dt_key))
                    usage_obj = {}
                    for i, header in enumerate(headers[2:]):
                        usage_obj[header] = data[i+2]
                    if logging:
                        log.write(str({"hour":dt_key, "data":usage_obj})+"\n")
                    db.usage.insert({"account":account, "month":mm_key, "usage":[{"hour":dt_key, "data":usage_obj}]})
                
        if from_miss: # this dt was from the misses array, so remove it now that we found a matching log.
            db.processor.update({"processor":"slogging"}, {"$pull":{"misses":dt}})
            
    else: # we do not have a log for this hour in this run.
        current_processor = db.processor.find_one({"processor":"slogging"})
        if "misses" in current_processor:
            if dt not in current_processor["misses"]: # current miss is not in the array already, add it.
                db.processor.update({"processor":"slogging"}, {"$push":{"misses":dt}})
            else: # it is already in misses, check if we should abandon it.
                if dt < datetime.datetime.now() - datetime.timedelta(hours=dt_miss_delta):
                    db.processor.update({"processor":"slogging"}, {"$pull":{"misses":dt}})
        else: # add a 'misses' array to the processor with this dt.
            db.processor.update({"processor":"slogging"}, {"$set":{"misses":[dt]}})
            
    if not from_miss:
        # move the dt pointer in the processor to the next dt that should be processed.
        next_dt = dt + datetime.timedelta(hours=1)
        db.processor.update({"processor":"slogging"}, {"$set":{"current_dt":next_dt}})


# START RUNTIME
# -------------
# setup the processor and start execution.
if len(log_names) > 0:
    processor = db.processor.find_one({"processor":"slogging"})
    # if we do not have an slogging processor, create one.
    if not processor:
        # grab the first file in the slogging csv sequence and establish the current_dt
        file_pieces = log_names[0].split('/')
        current_dt = datetime.datetime(int(file_pieces[0]), int(file_pieces[1]), int(file_pieces[2]), int(file_pieces[3]))
        db.processor.insert({"processor":"slogging", "current_dt":current_dt})
        processor = db.processor.find_one({"processor":"slogging"})
        # make sure there is a set of admin keys in the db.
        admin_keys = db.processor.find_one({"admin":"True"})
        if not admin_keys:
            keys = {}
            keys["label"] = "admin"
            keys["admin"] = "True"
            keys["api_key"] = hmac.new(key_salt, str(datetime.datetime.now()), hashlib.sha1).hexdigest()
            keys["secret_key"] = hmac.new(keys["api_key"], key_salt, hashlib.sha1).hexdigest()
            db.keys.insert({"api_key":keys["api_key"], "secret_key":keys["secret_key"], "label":keys["label"], "admin":keys["admin"]})
            log.write(str({"admin_keys":keys})+"\n")
        else:
            log.write(str({"admin_keys":admin_keys})+"\n")

    # process the previous misses
    if "misses" in processor:
        for dt in processor["misses"]:
            process_dt(dt, from_miss=True)

    # grab the processor again because it has likely changed after processing the misses.
    processor = db.processor.find_one({"processor":"slogging"})
    current_dt = processor["current_dt"]
    last_dt = datetime.datetime.now() - datetime.timedelta(hours=dt_now_delta)
    while current_dt <= last_dt:
        process_dt(current_dt)
        current_dt += datetime.timedelta(hours=1) # minimize db access by just incrementing the current dt with a local variable instead of accessing the db each time.
    
